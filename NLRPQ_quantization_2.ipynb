{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of nrlpq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meghbhalerao/nrlpq/blob/main/NLRPQ_quantization_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "xpS1yx2Xx2DW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b252c0f-07ef-4575-9d24-4d85570ec55d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Type, Any, Callable, Union, List, Optional\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from typing import Type, Any, Callable, Union, List, Optional\n",
        "\n",
        "\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
        "           'wide_resnet50_2', 'wide_resnet101_2']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
        "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
        "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
        "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion: int = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        # Rename relu to relu1\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.skip_add = nn.quantized.FloatFunctional()\n",
        "        # Remember to use two independent ReLU for layer fusion.\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "        \n",
        "        # Use FloatFunctional for addition for quantization compatibility\n",
        "        # out += identity\n",
        "        out = self.skip_add.add(identity, out)\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion: int = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.skip_add = nn.quantized.FloatFunctional()\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        # out += identity\n",
        "        out = self.skip_add.add(identity, out)\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        block: Type[Union[BasicBlock, Bottleneck]],\n",
        "        layers: List[int],\n",
        "        num_classes: int = 1000,\n",
        "        zero_init_residual: bool = False,\n",
        "        groups: int = 1,\n",
        "        width_per_group: int = 64,\n",
        "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
        "\n",
        "    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\n",
        "                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "def _resnet(\n",
        "    arch: str,\n",
        "    block: Type[Union[BasicBlock, Bottleneck]],\n",
        "    layers: List[int],\n",
        "    pretrained: bool,\n",
        "    progress: bool,\n",
        "    **kwargs: Any\n",
        ") -> ResNet:\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    r\"\"\"ResNet-18 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "def resnet34(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    r\"\"\"ResNet-34 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "def resnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    r\"\"\"ResNet-50 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "def resnet101(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    r\"\"\"ResNet-101 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "def resnet152(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    r\"\"\"ResNet-152 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "def resnext50_32x4d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    r\"\"\"ResNeXt-50 32x4d model from\n",
        "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['groups'] = 32\n",
        "    kwargs['width_per_group'] = 4\n",
        "    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],\n",
        "                   pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def resnext101_32x8d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    r\"\"\"ResNeXt-101 32x8d model from\n",
        "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['groups'] = 32\n",
        "    kwargs['width_per_group'] = 8\n",
        "    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],\n",
        "                   pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def wide_resnet50_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    r\"\"\"Wide ResNet-50-2 model from\n",
        "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_.\n",
        "\n",
        "    The model is the same as ResNet except for the bottleneck number of channels\n",
        "    which is twice larger in every block. The number of channels in outer 1x1\n",
        "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
        "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['width_per_group'] = 64 * 2\n",
        "    return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3],\n",
        "                   pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def wide_resnet101_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    r\"\"\"Wide ResNet-101-2 model from\n",
        "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_.\n",
        "\n",
        "    The model is the same as ResNet except for the bottleneck number of channels\n",
        "    which is twice larger in every block. The number of channels in outer 1x1\n",
        "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
        "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['width_per_group'] = 64 * 2\n",
        "    return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3],\n",
        "                   pretrained, progress, **kwargs)"
      ],
      "metadata": {
        "id": "I1L9-uYxPkov"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "#from resnet import resnet18\n",
        "\n",
        "def set_random_seeds(random_seed=0):\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(random_seed)\n",
        "    random.seed(random_seed)\n",
        "\n",
        "def prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256):\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    train_set = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=True, transform=train_transform) \n",
        "    # We will use test set for validation and test in this project.\n",
        "    # Do not use test set for validation in practice!\n",
        "    test_set = torchvision.datasets.CIFAR10(root=\"data\", train=False, download=True, transform=test_transform)\n",
        "\n",
        "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
        "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset=train_set, batch_size=train_batch_size,\n",
        "        sampler=train_sampler, num_workers=num_workers)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        dataset=test_set, batch_size=eval_batch_size,\n",
        "        sampler=test_sampler, num_workers=num_workers)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "def evaluate_model(model, test_loader, device, criterion=None):\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    running_loss = 0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in test_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        if criterion is not None:\n",
        "            loss = criterion(outputs, labels).item()\n",
        "        else:\n",
        "            loss = 0\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    eval_loss = running_loss / len(test_loader.dataset)\n",
        "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
        "\n",
        "    return eval_loss, eval_accuracy\n",
        "\n",
        "def train_model(model, train_loader, test_loader, device):\n",
        "\n",
        "    # The training configurations were not carefully selected.\n",
        "    learning_rate = 1e-2\n",
        "    num_epochs = 5\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10.\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "\n",
        "        print(\"Epoch: {:02d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(epoch, train_loss, train_accuracy, eval_loss, eval_accuracy))\n",
        "\n",
        "    return model\n",
        "\n",
        "def calibrate_model(model, loader, device=torch.device(\"cpu:0\")):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        _ = model(inputs)\n",
        "\n",
        "def measure_inference_latency(model,\n",
        "                              device,\n",
        "                              input_size=(1, 3, 32, 32),\n",
        "                              num_samples=100,\n",
        "                              num_warmups=10):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    x = torch.rand(size=input_size).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_warmups):\n",
        "            _ = model(x)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        for _ in range(num_samples):\n",
        "            _ = model(x)\n",
        "            torch.cuda.synchronize()\n",
        "        end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_time_ave = elapsed_time / num_samples\n",
        "\n",
        "    return elapsed_time_ave\n",
        "\n",
        "def save_model(model, model_dir, model_filename):\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.save(model.state_dict(), model_filepath)\n",
        "\n",
        "def load_model(model, model_filepath, device):\n",
        "\n",
        "    model.load_state_dict(torch.load(model_filepath, map_location=device))\n",
        "\n",
        "    return model\n",
        "\n",
        "def save_torchscript_model(model, model_dir, model_filename):\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.jit.save(torch.jit.script(model), model_filepath)\n",
        "\n",
        "def load_torchscript_model(model_filepath, device):\n",
        "\n",
        "    model = torch.jit.load(model_filepath, map_location=device)\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_model(num_classes=10):\n",
        "\n",
        "    # The number of channels in ResNet18 is divisible by 8.\n",
        "    # This is required for fast GEMM integer matrix multiplication.\n",
        "    # model = torchvision.models.resnet18(pretrained=False)\n",
        "    model = resnet50(pretrained=True)\n",
        "\n",
        "    # We would use the pretrained ResNet18 as a feature extractor.\n",
        "    # for param in model.parameters():\n",
        "    #     param.requires_grad = False\n",
        "    \n",
        "    # Modify the last FC layer\n",
        "    # num_features = model.fc.in_features\n",
        "    # model.fc = nn.Linear(num_features, 10)\n",
        "\n",
        "    return model\n",
        "\n",
        "class QuantizedResNet18(nn.Module):\n",
        "    def __init__(self, model_fp32):\n",
        "        super(QuantizedResNet18, self).__init__()\n",
        "        # QuantStub converts tensors from floating point to quantized.\n",
        "        # This will only be used for inputs.\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        # DeQuantStub converts tensors from quantized to floating point.\n",
        "        # This will only be used for outputs.\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "        # FP32 model\n",
        "        self.model_fp32 = model_fp32\n",
        "\n",
        "    def forward(self, x):\n",
        "        # manually specify where tensors will be converted from floating\n",
        "        # point to quantized in the quantized model\n",
        "        x = self.quant(x)\n",
        "        x = self.model_fp32(x)\n",
        "        # manually specify where tensors will be converted from quantized\n",
        "        # to floating point in the quantized model\n",
        "        x = self.dequant(x)\n",
        "        return x\n",
        "\n",
        "def model_equivalence(model_1, model_2, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,3,32,32)):\n",
        "\n",
        "    model_1.to(device)\n",
        "    model_2.to(device)\n",
        "\n",
        "    for _ in range(num_tests):\n",
        "        x = torch.rand(size=input_size).to(device)\n",
        "        y1 = model_1(x)[0].detach().cpu().numpy()\n",
        "        y2 = model_2(x)[0].detach().cpu().numpy()\n",
        "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
        "            print(\"Model equivalence test sample failed: \")\n",
        "            print(y1)\n",
        "            print(y2)\n",
        "            return False\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "4v6wKw7BP4Uv"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10"
      ],
      "metadata": {
        "id": "_xsmHEV2V405"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5SMuhdzQl6pb"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleHeadNestedLinear(nn.Linear):\n",
        "  def __init__(self, nesting_list: List, num_classes=num_classes, **kwargs):\n",
        "    super(SingleHeadNestedLinear, self).__init__(nesting_list[-1], num_classes, **kwargs)\n",
        "    self.nesting_list=nesting_list\n",
        "    self.num_classes=num_classes # Number of classes for classification\n",
        "    self.quant = torch.quantization.QuantStub()\n",
        "    self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "  def forward(self, x):\n",
        "    nesting_logits = ()\n",
        "    x = self.dequant(x)\n",
        "    for i, num_feat in enumerate(self.nesting_list):\n",
        "      if not (self.bias is None):\n",
        "        logit = torch.matmul(x[:, :num_feat], (self.weight[:, :num_feat]).t()) + self.bias\n",
        "      else:\n",
        "        logit = torch.matmul(x[:, :num_feat], (self.weight[:, :num_feat]).t())\n",
        "      nesting_logits+= (logit,)\n",
        "    x = self.quant(x)\n",
        "    return nesting_logits"
      ],
      "metadata": {
        "id": "t8nuXx8uVqTU"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = create_model(num_classes=num_classes)"
      ],
      "metadata": {
        "id": "2-PgAEIXXTsh"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nesting_start=3\n",
        "\n",
        "class BlurPoolConv2d(torch.nn.Module):\n",
        "    def __init__(self, conv):\n",
        "        super().__init__()\n",
        "        default_filter = torch.tensor([[[[1, 2, 1], [2, 4, 2], [1, 2, 1]]]]) / 16.0\n",
        "        filt = default_filter.repeat(conv.in_channels, 1, 1, 1)\n",
        "        self.conv = conv\n",
        "        self.register_buffer('blur_filter', filt)\n",
        "\n",
        "    def forward(self, x):\n",
        "        blurred = F.conv2d(x, self.blur_filter, stride=1, padding=(1, 1),\n",
        "                           groups=self.conv.in_channels, bias=None)\n",
        "        return self.conv.forward(blurred)\n",
        "\n",
        "class Model():\n",
        "    def __init__(self, gpu, nesting, single_head, fixed_feature, use_blurpool):\n",
        "        super().__init__()\n",
        "        self.gpu = gpu\n",
        "        self.nesting = nesting\n",
        "        self.sh = single_head\n",
        "        self.ff = fixed_feature\n",
        "        self.use_blurpool = use_blurpool\n",
        "\n",
        "\n",
        "    def load_model(self, model, model_weights_disk, modify_keys = True):\n",
        "        if os.path.isfile(model_weights_disk):\n",
        "            print(\"=> loading checkpoint '{}'\".format(model_weights_disk))\n",
        "            if self.gpu is None:\n",
        "                checkpoint = torch.load(model_weights_disk)\n",
        "            else:\n",
        "                # Map model to be loaded to specified single gpu.\n",
        "                loc = 'cuda:{}'.format(self.gpu)\n",
        "                checkpoint = torch.load(model_weights_disk, map_location=loc)\n",
        "            if modify_keys:\n",
        "              checkpoint = self.change_str_dict(checkpoint)\n",
        "            \n",
        "\n",
        "\n",
        "            try:\n",
        "              model.load_state_dict(checkpoint)\n",
        "            except:\n",
        "              proxy_layer = nn.Linear(2048, num_classes)\n",
        "              print(\"randomly init last fc layer\")\n",
        "              checkpoint[\"fc.bias\"] = proxy_layer.bias\n",
        "              checkpoint[\"fc.weight\"] = proxy_layer.weight\n",
        "              print(checkpoint.keys())\n",
        "              model.load_state_dict(checkpoint)\n",
        "\n",
        "            print(\"=> loaded checkpoint '{}' \"\n",
        "                  .format(model_weights_disk))\n",
        "        else:\n",
        "            print(\"=> no model found at '{}'\".format(model_weights_disk))\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "    def change_str_dict(self, x):\n",
        "      x_ = {}\n",
        "      for key, val in x.items():\n",
        "        key_new = str(key).replace(\"module.\",\"\")\n",
        "        x_[key_new] = copy.copy(val)\n",
        "      return x_\n",
        "\n",
        "    def initModel(self):\n",
        "        print(\"Model init: nesting=%d, sh=%d, ff=%d\" %(self.nesting, self.sh, self.ff))\n",
        "      #  model = models.resnet50(pretrained=True)\n",
        "        model = create_model(num_classes=num_classes)\n",
        "        nesting_list = [2**i for i in range(nesting_start, 12)] if self.nesting else None\n",
        "\n",
        "        # Nesting/Fixed Feature Modification code block\n",
        "        if self.nesting:\n",
        "            ff= \"Single Head\" if self.sh else \"Multi Head\"\n",
        "            print(\"Using Nesting of type - {}\".format(ff))\n",
        "            print(\"Nesting Starts from {}\".format(2**nesting_start))\n",
        "            if self.sh:\n",
        "                model.fc =  SingleHeadNestedLinear(nesting_list, num_classes=num_classes)\n",
        "            else:\n",
        "                model.fc =  MultiHeadNestedLinear(nesting_list, num_classes=num_classes)\n",
        "        elif self.ff != 2048:\n",
        "            print(f\"Using Fixed Features = {self.ff}\")\n",
        "            model.fc =  FixedFeatureLayer(self.ff, num_classes)\n",
        "\n",
        "        def apply_blurpool(mod: torch.nn.Module):\n",
        "            for (name, child) in mod.named_children():\n",
        "                if isinstance(child, torch.nn.Conv2d) and (np.max(child.stride) > 1 and child.in_channels >= 16):\n",
        "                    setattr(mod, name, BlurPoolConv2d(child))\n",
        "                else: apply_blurpool(child)\n",
        "        if self.use_blurpool: apply_blurpool(model)\n",
        "\n",
        "        model = model.to(memory_format=torch.channels_last)\n",
        "        model = model.to(self.gpu)\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Type, Any, Callable, Union, List, Optional\n",
        "\n",
        "# class SingleHeadNestedLinear(nn.Linear):\n",
        "# \tdef __init__(self, nesting_list: List, num_classes=num_classes, **kwargs):\n",
        "# \t\tsuper(SingleHeadNestedLinear, self).__init__(nesting_list[-1], num_classes, **kwargs)\n",
        "# \t\tself.nesting_list=nesting_list\n",
        "# \t\tself.num_classes=num_classes # Number of classes for classification\n",
        "\n",
        "# \tdef forward(self, x):\n",
        "# \t\tnesting_logits = ()\n",
        "# \t\tfor i, num_feat in enumerate(self.nesting_list):\n",
        "# \t\t\tif not (self.bias is None):\n",
        "# \t\t\t\tlogit = torch.matmul(x[:, :num_feat], (self.weight[:, :num_feat]).t()) + self.bias\n",
        "# \t\t\telse:\n",
        "# \t\t\t\tlogit = torch.matmul(x[:, :num_feat], (self.weight[:, :num_feat]).t())\n",
        "# \t\t\tnesting_logits+= (logit,)\n",
        "# \t\treturn nesting_logits\n",
        "\n",
        "class MultiHeadNestedLinear(nn.Module):\n",
        "\tdef __init__(self, nesting_list: List, num_classes=num_classes, **kwargs):\n",
        "\t\tsuper(MultiHeadNestedLinear, self).__init__()\n",
        "\t\tself.nesting_list=nesting_list\n",
        "\t\tself.num_classes=num_classes # Number of classes for classification\n",
        "\t\tfor i, num_feat in enumerate(self.nesting_list):\n",
        "\t\t\tsetattr(self, f\"nesting_classifier_{i}\", nn.Linear(num_feat, self.num_classes, **kwargs))\t\t\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tnesting_logits = ()\n",
        "\t\tfor i, num_feat in enumerate(self.nesting_list):\n",
        "\t\t\tnesting_logits +=  (getattr(self, f\"nesting_classifier_{i}\")(x[:, :num_feat]),)\n",
        "\t\treturn nesting_logits\n",
        "\n",
        "\t\t\n",
        "class FixedFeatureLayer(nn.Linear):\n",
        "    # This layer just takes the first \"K\" Features for the classification. \n",
        "    # Creating a separate layer and customized fwd pass helps to not change the base codes at all.\n",
        "    def __init__(self, in_features, out_features, **kwargs):\n",
        "        super(FixedFeatureLayer, self).__init__(in_features, out_features, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not (self.bias is None):\n",
        "            out = torch.matmul(x[:, :self.in_features], self.weight.t()) + self.bias\n",
        "        else:\n",
        "            out = torch.matmul(x[:, :self.in_features], self.weight.t())\n",
        "        return out\n",
        "\n",
        "class NestedCELoss(nn.Module):\n",
        "\tdef __init__(self, **kwargs):\n",
        "\t\tsuper(NestedCELoss, self).__init__()\n",
        "\t\tself.criterion = nn.CrossEntropyLoss(**kwargs)\n",
        "\tdef forward(self, output, target):\n",
        "\t\tloss=0\n",
        "\t\tfor o in output:\n",
        "\t\t\tloss+= self.criterion(o, target)\n",
        "\n",
        "\t\treturn loss"
      ],
      "metadata": {
        "id": "5GlfwMHQ4Voy"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def training_loop(model, criterion, optimizer, train_loader, valid_loader, epochs, device):\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "\n",
        "    for epoch in range(0, epochs):\n",
        "        # training\n",
        "        print(f'EPOCH:{epoch}')\n",
        "        print('Computing Train Loss..')\n",
        "        model, optimizer, train_loss = train(train_loader, model, criterion, optimizer, device)\n",
        "      \n",
        "        print(train_loss)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # validation\n",
        "        with torch.no_grad():\n",
        "            print('Computing Validation Loss..')\n",
        "            model, valid_loss = validate(valid_loader, model, criterion, device)\n",
        "            print(valid_loss)\n",
        "            valid_losses.append(valid_loss)\n",
        "        print('--------------------------------')\n",
        "  \n",
        "    return model, train_losses, valid_losses\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for X, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        pred = model(X) \n",
        "        loss = criterion(pred, y) \n",
        "        running_loss += loss.item() * X.size(0)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      #  print(loss.cpu().data.item())\n",
        "        \n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    return model, optimizer, epoch_loss\n",
        "\n",
        "def validate(valid_loader, model, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    correct_count, all_count = 0, 0\n",
        "    \n",
        "    for X, y in valid_loader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        pred = model(X) \n",
        "        \n",
        "        loss = criterion(pred, y) \n",
        "        running_loss += loss.item() * X.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(valid_loader.dataset)\n",
        "    return model, epoch_loss\n",
        "\n",
        "def get_accuracy(model,data_loader,rep_no, device):\n",
        "  correct_count, all_count = 0, 0\n",
        "  model.to(device)\n",
        "  for images,labels in data_loader:\n",
        "    images,labels = images.to(device), labels.to(device)\n",
        "    images = images.to(device)\n",
        "    true_label = labels.to(device)\n",
        "    pred = model(images) \n",
        "    pred = pred[rep_no]\n",
        "    pred_label = torch.argmax(pred, dim=1)\n",
        "    correct_count += torch.eq(true_label, pred_label).sum().item()\n",
        "    all_count +=len(true_label)\n",
        "  return correct_count/all_count\n",
        "\n",
        "def unabstract_model(model):\n",
        "  for no in range(2,5):\n",
        "    l1 = getattr(getattr(getattr(getattr(getattr(model,f'layer{no}'),'0'),'downsample'),'0'),'conv')\n",
        "    l2 = getattr(getattr(getattr(getattr(model,f'layer{no}'),'0'),'downsample'),'1')\n",
        "    setattr(getattr(getattr(model,f'layer{no}'),'0'),'downsample',nn.Sequential(l1,l2))\n",
        "\n",
        "    conv_layer = getattr(getattr(getattr(getattr(model,f'layer{no}'),'0'),'conv2'),'conv')\n",
        "    setattr(getattr(getattr(model,f'layer{no}'),'0'),'conv2',conv_layer)\n",
        "\n",
        "class QuantizedModel(nn.Module):\n",
        "    def __init__(self, model_fp32):\n",
        "        super(QuantizedModel, self).__init__()\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "        self.model_fp32 = model_fp32\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = ()\n",
        "        x = self.quant(x)\n",
        "        x = self.model_fp32(x)\n",
        "        for i in x:\n",
        "          res+=(self.dequant(i),)\n",
        "        return res\n",
        "\n",
        "def set_random_seeds(random_seed=0):\n",
        "    import random\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(random_seed)\n",
        "    random.seed(random_seed)"
      ],
      "metadata": {
        "id": "5Sv9k7-7p5DG"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mdl_wts = os.path.join(\"/content/gdrive/MyDrive/nrlpq/Imagenet1k_R50_sh1_mh0_ns3_ff2048/final_weights.pt\")\n",
        "model_wts_path = os.path.join(mdl_wts)\n",
        "nesting = 1\n",
        "single_head = 1\n",
        "fixed_feature = 2048\n",
        "model = Model(0, nesting, single_head, fixed_feature, use_blurpool=1)\n",
        "set_random_seeds(random_seed=0)\n",
        "model_init = model.initModel()\n",
        "model = model.load_model(model_init, model_wts_path)\n",
        "print(\"Loaded pretrained model: \" + str(model_wts_path))\n",
        "unabstract_model(model)\n",
        "batch_size = 128\n",
        "print(\"batch size is\", batch_size)\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset_all = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "n_alltrain = len(trainset_all)\n",
        "print(\"len of all train data is\", n_alltrain)\n",
        "\n",
        "n_train = int(n_alltrain * 0.8)\n",
        "n_val = n_alltrain - n_train\n",
        "print(\"len of train val split is \", n_train, n_val, \"respectively\")\n",
        "\n",
        "val_idxs = np.random.choice(n_alltrain, size = n_val ,replace=False)\n",
        "\n",
        "trainset = Subset(trainset_all, list(set(range(len(trainset_all))) -  set(val_idxs)))\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "valset = Subset(trainset_all, val_idxs)\n",
        "val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "print(\"length of train, val and test DataLoader is \", len(train_loader), len(val_loader), len(test_loader))\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = NestedCELoss()\n",
        "epochs = 1\n",
        "device = torch.device('cuda')\n",
        "\n",
        "import torch.quantization\n",
        "\n",
        "#quantized_model = torch.quantization.quantize_dynamic(model,dtype=torch.float16)\n",
        "\n",
        "model,_,_ = training_loop(model, criterion, optimizer, train_loader, val_loader, epochs, device)\n",
        "\n",
        "\n",
        "res = get_accuracy(model, test_loader,3, device)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdwXgxeQ_DNI",
        "outputId": "6413e51e-7e41-4afa-f919-9c75d226e2cd"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model init: nesting=1, sh=1, ff=2048\n",
            "Using Nesting of type - Single Head\n",
            "Nesting Starts from 8\n",
            "=> loading checkpoint '/content/gdrive/MyDrive/nrlpq/Imagenet1k_R50_sh1_mh0_ns3_ff2048/final_weights.pt'\n",
            "randomly init last fc layer\n",
            "dict_keys(['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.num_batches_tracked', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.bn3.bias', 'layer1.0.bn3.running_mean', 'layer1.0.bn3.running_var', 'layer1.0.bn3.num_batches_tracked', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.0.downsample.1.bias', 'layer1.0.downsample.1.running_mean', 'layer1.0.downsample.1.running_var', 'layer1.0.downsample.1.num_batches_tracked', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.num_batches_tracked', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.1.bn3.bias', 'layer1.1.bn3.running_mean', 'layer1.1.bn3.running_var', 'layer1.1.bn3.num_batches_tracked', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.bn1.bias', 'layer1.2.bn1.running_mean', 'layer1.2.bn1.running_var', 'layer1.2.bn1.num_batches_tracked', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.bn2.bias', 'layer1.2.bn2.running_mean', 'layer1.2.bn2.running_var', 'layer1.2.bn2.num_batches_tracked', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer1.2.bn3.bias', 'layer1.2.bn3.running_mean', 'layer1.2.bn3.running_var', 'layer1.2.bn3.num_batches_tracked', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.conv2.blur_filter', 'layer2.0.conv2.conv.weight', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.bn3.bias', 'layer2.0.bn3.running_mean', 'layer2.0.bn3.running_var', 'layer2.0.bn3.num_batches_tracked', 'layer2.0.downsample.0.blur_filter', 'layer2.0.downsample.0.conv.weight', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.0.downsample.1.num_batches_tracked', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.num_batches_tracked', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.1.bn3.bias', 'layer2.1.bn3.running_mean', 'layer2.1.bn3.running_var', 'layer2.1.bn3.num_batches_tracked', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.bn1.bias', 'layer2.2.bn1.running_mean', 'layer2.2.bn1.running_var', 'layer2.2.bn1.num_batches_tracked', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.bn2.bias', 'layer2.2.bn2.running_mean', 'layer2.2.bn2.running_var', 'layer2.2.bn2.num_batches_tracked', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.2.bn3.bias', 'layer2.2.bn3.running_mean', 'layer2.2.bn3.running_var', 'layer2.2.bn3.num_batches_tracked', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.bn1.bias', 'layer2.3.bn1.running_mean', 'layer2.3.bn1.running_var', 'layer2.3.bn1.num_batches_tracked', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.bn2.bias', 'layer2.3.bn2.running_mean', 'layer2.3.bn2.running_var', 'layer2.3.bn2.num_batches_tracked', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer2.3.bn3.bias', 'layer2.3.bn3.running_mean', 'layer2.3.bn3.running_var', 'layer2.3.bn3.num_batches_tracked', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.conv2.blur_filter', 'layer3.0.conv2.conv.weight', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.bn3.bias', 'layer3.0.bn3.running_mean', 'layer3.0.bn3.running_var', 'layer3.0.bn3.num_batches_tracked', 'layer3.0.downsample.0.blur_filter', 'layer3.0.downsample.0.conv.weight', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.0.downsample.1.num_batches_tracked', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.num_batches_tracked', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.1.bn3.bias', 'layer3.1.bn3.running_mean', 'layer3.1.bn3.running_var', 'layer3.1.bn3.num_batches_tracked', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn1.bias', 'layer3.2.bn1.running_mean', 'layer3.2.bn1.running_var', 'layer3.2.bn1.num_batches_tracked', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.bn2.bias', 'layer3.2.bn2.running_mean', 'layer3.2.bn2.running_var', 'layer3.2.bn2.num_batches_tracked', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.2.bn3.bias', 'layer3.2.bn3.running_mean', 'layer3.2.bn3.running_var', 'layer3.2.bn3.num_batches_tracked', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.bn1.bias', 'layer3.3.bn1.running_mean', 'layer3.3.bn1.running_var', 'layer3.3.bn1.num_batches_tracked', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.bn2.bias', 'layer3.3.bn2.running_mean', 'layer3.3.bn2.running_var', 'layer3.3.bn2.num_batches_tracked', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.3.bn3.bias', 'layer3.3.bn3.running_mean', 'layer3.3.bn3.running_var', 'layer3.3.bn3.num_batches_tracked', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.bn1.bias', 'layer3.4.bn1.running_mean', 'layer3.4.bn1.running_var', 'layer3.4.bn1.num_batches_tracked', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.bn2.bias', 'layer3.4.bn2.running_mean', 'layer3.4.bn2.running_var', 'layer3.4.bn2.num_batches_tracked', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.4.bn3.bias', 'layer3.4.bn3.running_mean', 'layer3.4.bn3.running_var', 'layer3.4.bn3.num_batches_tracked', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn1.bias', 'layer3.5.bn1.running_mean', 'layer3.5.bn1.running_var', 'layer3.5.bn1.num_batches_tracked', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn2.bias', 'layer3.5.bn2.running_mean', 'layer3.5.bn2.running_var', 'layer3.5.bn2.num_batches_tracked', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer3.5.bn3.bias', 'layer3.5.bn3.running_mean', 'layer3.5.bn3.running_var', 'layer3.5.bn3.num_batches_tracked', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.num_batches_tracked', 'layer4.0.conv2.blur_filter', 'layer4.0.conv2.conv.weight', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.num_batches_tracked', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.bn3.bias', 'layer4.0.bn3.running_mean', 'layer4.0.bn3.running_var', 'layer4.0.bn3.num_batches_tracked', 'layer4.0.downsample.0.blur_filter', 'layer4.0.downsample.0.conv.weight', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.0.downsample.1.running_mean', 'layer4.0.downsample.1.running_var', 'layer4.0.downsample.1.num_batches_tracked', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.num_batches_tracked', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.num_batches_tracked', 'layer4.1.conv3.weight', 'layer4.1.bn3.weight', 'layer4.1.bn3.bias', 'layer4.1.bn3.running_mean', 'layer4.1.bn3.running_var', 'layer4.1.bn3.num_batches_tracked', 'layer4.2.conv1.weight', 'layer4.2.bn1.weight', 'layer4.2.bn1.bias', 'layer4.2.bn1.running_mean', 'layer4.2.bn1.running_var', 'layer4.2.bn1.num_batches_tracked', 'layer4.2.conv2.weight', 'layer4.2.bn2.weight', 'layer4.2.bn2.bias', 'layer4.2.bn2.running_mean', 'layer4.2.bn2.running_var', 'layer4.2.bn2.num_batches_tracked', 'layer4.2.conv3.weight', 'layer4.2.bn3.weight', 'layer4.2.bn3.bias', 'layer4.2.bn3.running_mean', 'layer4.2.bn3.running_var', 'layer4.2.bn3.num_batches_tracked', 'fc.weight', 'fc.bias'])\n",
            "=> loaded checkpoint '/content/gdrive/MyDrive/nrlpq/Imagenet1k_R50_sh1_mh0_ns3_ff2048/final_weights.pt' \n",
            "Loaded pretrained model: /content/gdrive/MyDrive/nrlpq/Imagenet1k_R50_sh1_mh0_ns3_ff2048/final_weights.pt\n",
            "batch size is 128\n",
            "Files already downloaded and verified\n",
            "len of all train data is 50000\n",
            "len of train val split is  40000 10000 respectively\n",
            "Files already downloaded and verified\n",
            "length of train, val and test DataLoader is  313 79 79\n",
            "EPOCH:0\n",
            "Computing Train Loss..\n",
            "12.964050215148927\n",
            "Computing Validation Loss..\n",
            "7.8153953262329106\n",
            "--------------------------------\n",
            "0.748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#layers = [[\"conv1\", \"bn1\"], [\"conv2\", \"bn2\"], [\"conv3\", \"bn3\", 'relu']]\n",
        "cpu_device = torch.device(\"cpu:0\")\n",
        "model.to(cpu_device)\n",
        "    # Make a copy of the model for layer fusion\n",
        "fused_model = copy.deepcopy(model)\n",
        "\n",
        "model.eval()\n",
        "# The model has to be switched to evaluation mode before any layer fusion.\n",
        "# Otherwise the quantization will not work correctly.\n",
        "fused_model.eval()\n",
        "\n",
        "# Fuse the model in place rather manually.\n",
        "fused_model = torch.quantization.fuse_modules(fused_model, [[\"conv1\", \"bn1\", \"relu\"]], inplace=True)\n",
        "for module_name, module in fused_model.named_children():\n",
        "    if \"layer\" in module_name:\n",
        "        for basic_block_name, basic_block in module.named_children():\n",
        "            torch.quantization.fuse_modules(basic_block, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\"]], inplace=True)\n",
        "            for sub_block_name, sub_block in basic_block.named_children():\n",
        "                if sub_block_name == \"downsample\":\n",
        "                    torch.quantization.fuse_modules(sub_block, [[\"0\", \"1\"]], inplace=True)\n",
        "\n",
        "# Model and fused model should be equivalent.\n",
        "assert model_equivalence(model_1=model, model_2=fused_model, device=cpu_device, rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,32,32)), \"Fused model is not equivalent to the original model!\"\n"
      ],
      "metadata": {
        "id": "geiY5FEOG2VI"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_accuracy(fused_model, test_loader,device=device,rep_no=8)"
      ],
      "metadata": {
        "id": "v8skJfTvGy11",
        "outputId": "3dc23103-fc3a-44a0-ff8c-b97edd0d2745",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7475"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.cpu()\n",
        "model.eval()\n",
        "fused_model = copy.deepcopy(model)\n",
        "fused_model.eval()\n",
        "\n",
        "quantized_model = QuantizedModel(model_fp32=fused_model)\n",
        "quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "quantized_model.qconfig = quantization_config\n",
        "torch.quantization.prepare(quantized_model, inplace=True)\n",
        "#quantized_model = quantized_model.cpu()\n",
        "quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
        "quantized_model.eval()\n",
        "print('Finished Quantization!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Wye8TMZHeE_",
        "outputId": "2299cd4b-a50e-4966-acd1-1eb8127b6597"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:179: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  reduce_range will be deprecated in a future release of PyTorch.\"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:1126: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
            "  Returning default scale and zero point \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Quantization!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get_accuracy(quantized_model, test_loader,8, device=torch.device(\"cpu:0\"))\n",
        "\n",
        "calibrate_model(quantized_model, train_loader, device=torch.device(\"cpu:0\"))"
      ],
      "metadata": {
        "id": "t47Y_XgjHjYO"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_accuracy(model,data_loader,rep_no, device):\n",
        "#   correct_count, all_count = 0, 0\n",
        "#   model.to(device)\n",
        "#   for images,labels in data_loader:\n",
        "#     images,labels = images.to(device), labels.to(device)\n",
        "#     images = images.to(device)\n",
        "#     true_label = labels.to(device)\n",
        "#     pred = model(images) \n",
        "#     print(pred)\n",
        "#     pred = pred[rep_no]\n",
        "#     pred_label = torch.argmax(pred, dim=1)\n",
        "#     correct_count += torch.eq(true_label, pred_label).sum().item()\n",
        "#     all_count +=len(true_label)\n",
        "#   return correct_count/all_count"
      ],
      "metadata": {
        "id": "hrsv0-5xC1ph"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_accuracy(fused_model, test_loader,device=device,rep_no=8)"
      ],
      "metadata": {
        "id": "FM2TdSwsEiQO",
        "outputId": "2984d5aa-4d5b-4cd9-df4b-dbd7c83a3894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        }
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-126-f02c4a79d9db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfused_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrep_no\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-108-5d4da5640b76>\u001b[0m in \u001b[0;36mget_accuracy\u001b[0;34m(model, data_loader, rep_no, device)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mtrue_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrep_no\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-caab441d932f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-caab441d932f>\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/quantized/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m                           mode=self.padding_mode)\n\u001b[1;32m    456\u001b[0m         return ops.quantized.conv2d(\n\u001b[0;32m--> 457\u001b[0;31m             input, self._packed_params, self.scale, self.zero_point)\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'quantized::conv2d.new' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d.new' is only available for these backends: [QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, Tracer, AutocastCPU, Autocast, Batched, VmapMode, Functionalize].\n\nQuantizedCPU: registered at ../aten/src/ATen/native/quantized/cpu/qconv.cpp:939 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:35 [backend fallback]\nAutogradCPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:39 [backend fallback]\nAutogradCUDA: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:47 [backend fallback]\nAutogradXLA: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:51 [backend fallback]\nAutogradLazy: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:55 [backend fallback]\nAutogradXPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:43 [backend fallback]\nAutogradMLC: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:59 [backend fallback]\nAutogradHPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:68 [backend fallback]\nTracer: registered at ../torch/csrc/autograd/TraceTypeManual.cpp:293 [backend fallback]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:461 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1059 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:52 [backend fallback]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in test_loader:\n",
        "  print(model(x)[0])"
      ],
      "metadata": {
        "id": "iRsAHXf0Fsxf",
        "outputId": "0d62300d-cacc-4946-9cf2-6e86f64c0791",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0284, -0.1537,  0.1416,  ..., -0.1181, -0.0895, -0.1646],\n",
            "        [ 1.4247, -0.3740, -2.0389,  ..., -1.9012,  4.0577, -0.2262],\n",
            "        [ 0.5788,  0.2308, -1.4222,  ..., -1.4853,  2.2316,  0.0937],\n",
            "        ...,\n",
            "        [ 1.1982, -0.5548, -0.0931,  ..., -0.5742,  0.2988,  0.1647],\n",
            "        [ 1.1485, -0.5417, -1.0226,  ..., -0.9186,  2.5166, -0.4781],\n",
            "        [-0.0916, -0.1984,  0.0763,  ..., -0.0088, -0.1780, -0.1004]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-0.0732, -0.1477,  0.1244,  ..., -0.1794, -0.1325, -0.1446],\n",
            "        [ 2.2120, -0.8683,  0.0710,  ..., -0.8560, -0.3169,  0.7164],\n",
            "        [-0.9290,  0.6611, -0.5405,  ...,  2.0164, -1.5534, -0.6370],\n",
            "        ...,\n",
            "        [-0.3692,  0.3040, -1.7104,  ..., -0.7996, -0.3560,  1.6788],\n",
            "        [-0.4767, -0.0934, -0.0303,  ..., -0.7316, -0.5196,  0.0360],\n",
            "        [ 0.9050, -0.4588, -0.0181,  ..., -0.4426,  0.1822,  0.0850]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-0.1109, -0.1426,  0.1100,  ..., -0.2310, -0.1687, -0.1277],\n",
            "        [ 1.9769, -0.8174, -0.5844,  ..., -1.0247,  1.3234,  0.1606],\n",
            "        [ 0.8926, -0.3334, -1.1040,  ..., -1.0958,  2.3737, -0.2588],\n",
            "        ...,\n",
            "        [-0.0535,  0.5186, -1.2490,  ..., -1.1644,  0.4067,  0.9382],\n",
            "        [ 2.7297, -1.0672,  0.2043,  ..., -0.9678, -0.6859,  0.9996],\n",
            "        [ 1.1869,  0.0864, -1.2289,  ..., -0.9869,  0.0732,  1.1127]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [ 2.5648, -0.9874, -0.3397,  ..., -1.1353,  0.5914,  0.5920],\n",
            "        [-2.1897, -0.5493, -1.8412,  ...,  4.0663, -2.6659,  0.6907],\n",
            "        ...,\n",
            "        [-0.6425,  0.4962, -0.3205,  ...,  1.4269, -1.1691, -0.5550],\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [-1.3420,  0.0230, -0.3622,  ..., -1.9162, -1.3501,  0.4232]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-1.3563,  0.0249, -0.3676,  ..., -1.9357, -1.3638,  0.4296],\n",
            "        [-1.2359,  2.9981, -1.8390,  ..., -0.7235, -0.9274,  0.6081],\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        ...,\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [ 2.3645, -0.9468, -0.6538,  ..., -1.1820,  1.4153,  0.2776],\n",
            "        [-0.4060, -0.1636, -0.3583,  ...,  0.5000, -0.5853,  0.2139]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-0.0660, -0.1486,  0.1272,  ..., -0.1695, -0.1256, -0.1478],\n",
            "        [-0.8326, -0.0455, -0.1668,  ..., -1.2188, -0.8612,  0.1952],\n",
            "        [ 1.1788, -0.5502, -0.0621,  ..., -0.5518,  0.2354,  0.1709],\n",
            "        ...,\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [-0.6673, -0.2622, -0.6672,  ...,  1.7017, -1.3823,  0.2069],\n",
            "        [ 0.2930, -0.1291, -0.6851,  ..., -0.2965, -0.1890,  0.5941]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [-0.6399,  0.4173, -0.3246,  ...,  1.3754, -1.0988, -0.5020],\n",
            "        [-0.5786, -0.3418, -0.4196,  ...,  0.9241, -0.7262,  0.2052],\n",
            "        ...,\n",
            "        [-0.1111, -0.2396,  0.0351,  ...,  0.1258, -0.2197, -0.0621],\n",
            "        [-1.0751,  2.3108, -1.5152,  ..., -0.8508, -0.5537,  0.6198],\n",
            "        [-0.7293,  0.3485, -1.7147,  ..., -0.6768, -0.7534,  1.6491]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-0.1694, -0.1347,  0.0876,  ..., -0.3110, -0.2248, -0.1016],\n",
            "        [-1.8356, -0.1618, -1.4430,  ...,  3.4204, -2.2179,  0.2242],\n",
            "        [-0.5820, -0.3710, -0.4179,  ...,  0.9982, -0.7714,  0.1984],\n",
            "        ...,\n",
            "        [-1.0071,  2.6820, -1.6996,  ..., -1.1757, -0.5372,  0.7782],\n",
            "        [ 2.5544, -1.0100, -0.5719,  ..., -1.2136,  1.1856,  0.4118],\n",
            "        [ 0.4551, -0.3084,  0.1890,  ..., -0.2104, -0.2243,  0.0330]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-0.4397, -0.0984, -0.0161,  ..., -0.6811, -0.4842,  0.0194],\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [ 2.0475, -0.8397,  0.1612,  ..., -0.7557, -0.4563,  0.6842],\n",
            "        ...,\n",
            "        [-1.5072,  0.2111, -1.0907,  ...,  2.8906, -1.9238, -0.1793],\n",
            "        [-1.2871,  0.0508, -0.9357,  ...,  2.4559, -1.6683, -0.0697],\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[ 2.1145, -0.8632, -0.5346,  ..., -1.0513,  1.1797,  0.2514],\n",
            "        [ 0.8918, -0.2508, -1.3345,  ..., -1.3412,  2.6623, -0.1562],\n",
            "        [-1.1197, -0.0069, -0.2769,  ..., -1.6119, -1.1368,  0.3238],\n",
            "        ...,\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [ 0.4813, -0.0860, -0.9644,  ..., -1.0910,  1.7951, -0.0263]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-0.6840, -0.3873, -0.6273,  ...,  1.1255, -1.1001,  0.4059],\n",
            "        [ 1.4239, -0.4666, -1.7768,  ..., -1.6228,  3.7279, -0.3416],\n",
            "        [-0.2698,  0.5345, -2.0504,  ..., -1.1523,  0.1701,  1.8210],\n",
            "        ...,\n",
            "        [ 0.5764,  0.3310, -0.9303,  ..., -1.1907,  0.5944,  0.6300],\n",
            "        [-0.5176,  0.1214, -1.7674,  ..., -0.4441, -1.0125,  2.0682],\n",
            "        [ 1.2867, -0.5406, -1.2981,  ..., -1.1624,  2.9950, -0.4552]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-1.3558,  3.5377, -2.1679,  ..., -0.9871, -0.9969,  0.7959],\n",
            "        [ 2.1614, -0.8776,  0.2172,  ..., -0.7720, -0.6102,  0.7692],\n",
            "        [-1.7061,  4.6515, -2.8226,  ..., -1.1859, -1.3371,  1.0282],\n",
            "        ...,\n",
            "        [-1.8277, -0.3591, -1.4827,  ...,  3.3926, -2.2262,  0.4213],\n",
            "        [-0.2432,  0.1471, -0.0233,  ...,  0.5341, -0.5305, -0.3602],\n",
            "        [-0.8374,  0.5231, -0.4769,  ...,  1.7772, -1.3570, -0.5531]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-1.6127e-01, -1.3583e-01,  9.0674e-02,  ..., -2.9990e-01,\n",
            "         -2.1699e-01, -1.0519e-01],\n",
            "        [-6.1068e-01,  2.8061e-01, -3.1157e-01,  ...,  1.2438e+00,\n",
            "         -9.5620e-01, -4.1258e-01],\n",
            "        [ 7.1671e-02, -1.6887e-01,  1.0911e-01,  ..., -1.3505e-01,\n",
            "          2.2115e-04, -1.4493e-01],\n",
            "        ...,\n",
            "        [-1.2277e+00,  7.6209e-03, -3.1830e-01,  ..., -1.7596e+00,\n",
            "         -1.2404e+00,  3.7207e-01],\n",
            "        [-3.4077e-01, -3.7859e-01, -2.1241e-01,  ...,  5.2512e-01,\n",
            "         -4.7964e-01,  1.6027e-01],\n",
            "        [-9.2875e-01, -3.2589e-02, -2.0366e-01,  ..., -1.3505e+00,\n",
            "         -9.5350e-01,  2.3829e-01]], grad_fn=<AddBackward0>)\n",
            "tensor([[ 2.0782, -0.8506, -0.2454,  ..., -0.9256,  0.5013,  0.4277],\n",
            "        [-1.2748,  0.0140, -0.3364,  ..., -1.8242, -1.2856,  0.3932],\n",
            "        [-0.8757, -0.0397, -0.1833,  ..., -1.2778, -0.9026,  0.2145],\n",
            "        ...,\n",
            "        [-0.0930, -0.2332,  0.0488,  ...,  0.1181, -0.1977, -0.0762],\n",
            "        [-1.1477, -0.2606, -0.8748,  ...,  2.1260, -1.4515,  0.1853],\n",
            "        [-0.9678, -0.0273, -0.2186,  ..., -1.4039, -0.9910,  0.2558]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-0.3312,  0.2716, -1.9376,  ..., -0.7835, -0.0855,  1.8171],\n",
            "        [-0.0740, -0.2186,  0.0693,  ...,  0.0809, -0.1705, -0.0963],\n",
            "        [-0.0560, -0.2047,  0.0889,  ...,  0.0456, -0.1446, -0.1154],\n",
            "        ...,\n",
            "        [ 1.0523, -0.3975, -1.2385,  ..., -1.1822,  2.6956, -0.3136],\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [-0.9342,  0.5405, -0.5543,  ...,  1.9535, -1.4538, -0.5549]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [ 0.6585, -0.3762,  0.2047,  ..., -0.2725, -0.2994,  0.1289],\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        ...,\n",
            "        [-0.7272,  1.6254, -1.0013,  ..., -0.5076, -0.5026,  0.3124],\n",
            "        [-0.0546, -0.2037,  0.0903,  ...,  0.0429, -0.1426, -0.1168],\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[ 1.4066, -0.4512, -1.7860,  ..., -1.6387,  3.7228, -0.3252],\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [ 1.4214, -0.3480, -2.1056,  ..., -1.9733,  4.1384, -0.1943],\n",
            "        ...,\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [-0.3169,  0.4641, -1.8394,  ..., -0.9724,  0.2673,  1.4991],\n",
            "        [-0.5939, -0.2418, -0.3949,  ...,  1.0925, -0.8296,  0.0564]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-0.8968,  2.1308, -1.3886,  ..., -0.9955, -0.4092,  0.6391],\n",
            "        [-0.9348,  0.4473, -2.0407,  ..., -0.6352, -1.0209,  2.1829],\n",
            "        [-0.4282, -0.1302, -0.0351,  ..., -0.5537, -0.4890,  0.0348],\n",
            "        ...,\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [ 2.9421, -1.1384,  0.0187,  ..., -1.1119, -0.2859,  0.9660],\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-1.0358, -0.0182, -0.2447,  ..., -1.4970, -1.0562,  0.2862],\n",
            "        [ 0.7214, -0.2417, -0.3168,  ..., -0.6673,  0.5579,  0.1099],\n",
            "        [-0.0819, -0.1465,  0.1211,  ..., -0.1913, -0.1408, -0.1407],\n",
            "        ...,\n",
            "        [ 1.4774, -0.5171, -1.7404,  ..., -1.5651,  3.7334, -0.3961],\n",
            "        [-1.8048, -0.4455, -1.4842,  ...,  3.3471, -2.2133,  0.5054],\n",
            "        [ 1.5717, -0.6043, -1.6811,  ..., -1.4687,  3.7493, -0.4899]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[ 0.3217,  0.5509, -1.1508,  ..., -1.4713,  0.9764,  0.6419],\n",
            "        [-0.6539,  0.2442, -2.2385,  ..., -0.3929, -1.0826,  2.5655],\n",
            "        [ 0.9690, -0.2960, -0.8496,  ..., -0.9474,  1.7478, -0.1472],\n",
            "        ...,\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [-0.8410,  0.5076, -0.4810,  ...,  1.7741, -1.3466, -0.5423],\n",
            "        [-1.3730, -0.4002, -1.0968,  ...,  2.5231, -1.6897,  0.3615]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-1.0759, -0.4749, -0.8472,  ...,  1.9222, -1.2901,  0.3521],\n",
            "        [-1.1057, -0.0088, -0.2715,  ..., -1.5928, -1.1234,  0.3175],\n",
            "        [-0.3621,  0.2100, -0.1150,  ...,  0.7754, -0.6852, -0.3905],\n",
            "        ...,\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [-2.3135, -0.5712, -1.9556,  ...,  4.3060, -2.8240,  0.7434],\n",
            "        [ 1.0866, -0.2751, -1.6504,  ..., -1.6080,  3.2459, -0.1559]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [-0.5988, -0.0876, -0.1723,  ..., -0.6362, -0.6262,  0.1552],\n",
            "        [ 0.2251, -0.2317,  0.1713,  ..., -0.1402, -0.1393, -0.0754],\n",
            "        ...,\n",
            "        [-1.1910,  2.9053, -1.8059,  ..., -0.8425, -0.8177,  0.6578],\n",
            "        [-1.0256,  1.0243, -1.5602,  ..., -1.0180, -0.3686,  1.3466],\n",
            "        [-0.1307, -0.1608,  0.0221,  ..., -0.0553, -0.1573, -0.0475]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-1.8341, -0.4431, -1.5084,  ...,  3.4004, -2.2431,  0.5079],\n",
            "        [-0.7750,  0.2424, -2.0848,  ..., -0.4566, -1.0611,  2.3432],\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        ...,\n",
            "        [-0.1338, -0.2646,  0.0047,  ...,  0.1978, -0.2561, -0.0330],\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [-0.0165,  0.7541, -1.2744,  ..., -1.4980,  1.3103,  0.4456]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-0.5733,  0.2129, -2.0872,  ..., -0.3803, -1.0676,  2.4023],\n",
            "        [-1.0613, -0.1227, -0.7721,  ...,  1.9869, -1.3665,  0.0378],\n",
            "        [-0.5497,  0.0674, -2.0063,  ..., -0.2514, -1.0897,  2.3519],\n",
            "        ...,\n",
            "        [-0.0468, -0.1977,  0.0987,  ...,  0.0278, -0.1315, -0.1250],\n",
            "        [ 1.8272, -0.7660,  0.2766,  ..., -0.6365, -0.6886,  0.6680],\n",
            "        [-2.0154, -0.4321, -1.6514,  ...,  3.7058, -2.3841,  0.5115]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[ 0.4680,  0.0571, -0.9955,  ..., -1.0969,  1.6936, -0.0101],\n",
            "        [-0.0472, -0.1512,  0.1344,  ..., -0.1438, -0.1075, -0.1562],\n",
            "        [-1.4227,  3.5164, -2.1869,  ..., -0.9643, -0.9901,  0.8052],\n",
            "        ...,\n",
            "        [-1.0895, -0.0110, -0.2653,  ..., -1.5705, -1.1078,  0.3102],\n",
            "        [-0.9674, -0.0274, -0.2185,  ..., -1.4034, -0.9906,  0.2556],\n",
            "        [-1.1431, -0.0038, -0.2859,  ..., -1.6439, -1.1592,  0.3342]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-0.0378, -0.1908,  0.1084,  ...,  0.0102, -0.1186, -0.1345],\n",
            "        [-0.1004, -0.1440,  0.1140,  ..., -0.2165, -0.1586, -0.1324],\n",
            "        [-0.2198, -0.2413, -0.0676,  ...,  0.3736, -0.3749, -0.0309],\n",
            "        ...,\n",
            "        [ 0.0164, -0.1621,  0.1447,  ..., -0.0806, -0.0372, -0.1808],\n",
            "        [-0.5538,  0.4435, -0.2525,  ...,  1.2432, -1.0486, -0.5286],\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[ 0.2439, -0.2379,  0.1727,  ..., -0.1459, -0.1462, -0.0666],\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [-0.6131, -0.3835, -0.4382,  ...,  1.0971, -0.8242,  0.1892],\n",
            "        ...,\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [-0.4959, -0.1384, -0.0744,  ..., -0.5827, -0.5630,  0.0767]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-7.8893e-01, -5.1397e-02, -1.5004e-01,  ..., -1.1591e+00,\n",
            "         -8.1933e-01,  1.7572e-01],\n",
            "        [-1.4037e-01,  1.3160e-01,  5.9184e-02,  ...,  3.4841e-01,\n",
            "         -4.3020e-01, -3.6035e-01],\n",
            "        [ 4.6210e-03, -1.5815e-01,  1.5429e-01,  ..., -7.2816e-02,\n",
            "         -5.7787e-02, -1.7944e-01],\n",
            "        ...,\n",
            "        [-4.7898e-01,  3.0006e-01, -2.0294e-01,  ...,  1.0295e+00,\n",
            "         -8.6166e-01, -4.3931e-01],\n",
            "        [-4.0012e-01, -1.0370e-01, -9.2576e-04,  ..., -6.2685e-01,\n",
            "         -4.4620e-01,  1.7017e-03],\n",
            "        [-1.2936e+00,  3.3678e+00, -2.0895e+00,  ..., -1.0766e+00,\n",
            "         -8.7224e-01,  8.2321e-01]], grad_fn=<AddBackward0>)\n",
            "tensor([[-0.0249, -0.1542,  0.1430,  ..., -0.1132, -0.0861, -0.1662],\n",
            "        [ 1.4552, -0.4733, -1.8199,  ..., -1.6575,  3.8120, -0.3450],\n",
            "        [ 1.5533, -0.5014, -1.9341,  ..., -1.7442,  4.0494, -0.3647],\n",
            "        ...,\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [ 1.0091, -0.4932,  0.2277,  ..., -0.3811, -0.4196,  0.2916],\n",
            "        [-0.9122, -0.0348, -0.1973,  ..., -1.3278, -0.9376,  0.2309]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-0.5253,  0.2468, -0.2448,  ...,  1.0772, -0.8549, -0.3985],\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        ...,\n",
            "        [-0.3609,  0.5316, -1.4998,  ..., -1.0462,  0.1717,  1.2131],\n",
            "        [-0.9977,  2.4507, -1.5678,  ..., -1.0106, -0.5328,  0.6830],\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-3.8059e-01, -1.0633e-01,  6.5644e-03,  ..., -6.0011e-01,\n",
            "         -4.2745e-01, -7.0392e-03],\n",
            "        [ 1.3283e-01,  1.8930e-01, -1.6714e+00,  ..., -8.8161e-01,\n",
            "         -2.0004e-03,  1.5873e+00],\n",
            "        [ 1.0918e+00, -3.2178e-01, -1.5292e+00,  ..., -1.4773e+00,\n",
            "          3.0986e+00, -2.1322e-01],\n",
            "        ...,\n",
            "        [ 2.5378e+00, -9.7354e-01, -4.8577e-01,  ..., -1.1854e+00,\n",
            "          9.3408e-01,  4.8832e-01],\n",
            "        [-6.4566e-01,  5.6161e-01, -3.1791e-01,  ...,  1.4712e+00,\n",
            "         -1.2281e+00, -5.9893e-01],\n",
            "        [-3.3596e-01,  2.1769e-01, -9.3108e-02,  ...,  7.3516e-01,\n",
            "         -6.6975e-01, -3.9838e-01]], grad_fn=<AddBackward0>)\n",
            "tensor([[ 0.7788, -0.3110, -0.9424,  ..., -0.9644,  2.0616, -0.2487],\n",
            "        [ 0.0328, -0.1904,  0.0261,  ..., -0.0987, -0.1654,  0.0273],\n",
            "        [-0.4874, -0.1985, -0.9484,  ...,  0.5785, -1.0416,  0.9244],\n",
            "        ...,\n",
            "        [-0.0784, -0.2219,  0.0647,  ...,  0.0894, -0.1767, -0.0917],\n",
            "        [-0.2097, -0.3210, -0.0760,  ...,  0.3436, -0.3599,  0.0437],\n",
            "        [ 0.6298,  0.1299, -1.3713,  ..., -1.4254,  2.2657,  0.0373]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[ 1.4671, -0.6469, -0.3504,  ..., -0.7617,  0.8644,  0.1012],\n",
            "        [-0.1274, -0.1404,  0.1037,  ..., -0.2535, -0.1845, -0.1203],\n",
            "        [-0.1698,  0.3775, -1.5084,  ..., -0.9421,  0.4151,  1.1342],\n",
            "        ...,\n",
            "        [ 0.3230, -0.2643,  0.1788,  ..., -0.1700, -0.1754, -0.0293],\n",
            "        [ 1.1521, -0.3866, -1.4659,  ..., -1.3886,  3.0768, -0.2845],\n",
            "        [ 1.3746, -0.6175, -1.2553,  ..., -1.0858,  3.0256, -0.5372]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-0.1864, -0.2850, -0.0483,  ...,  0.3052, -0.3356,  0.0045],\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [ 1.4852, -0.6546, -1.3691,  ..., -1.1675,  3.2744, -0.5660],\n",
            "        ...,\n",
            "        [ 2.0194, -0.8320, -0.8442,  ..., -1.1409,  1.9310,  0.0063],\n",
            "        [-0.6897,  0.0423, -0.4228,  ...,  1.3328, -0.9885, -0.1839],\n",
            "        [ 1.3871, -0.4808, -1.6644,  ..., -1.5163,  3.5513, -0.3650]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-0.0837, -0.2260,  0.0589,  ...,  0.0998, -0.1843, -0.0861],\n",
            "        [-1.0742, -0.0130, -0.2594,  ..., -1.5495, -1.0930,  0.3034],\n",
            "        [-0.1544, -0.1466,  0.0697,  ..., -0.2227, -0.2052, -0.0873],\n",
            "        ...,\n",
            "        [-0.2099, -0.1293,  0.0720,  ..., -0.3665, -0.2637, -0.0834],\n",
            "        [-0.6699,  1.5729, -1.1370,  ..., -1.3688,  0.0456,  0.7649],\n",
            "        [-2.1259, -0.4940, -1.7676,  ...,  3.9310, -2.5542,  0.6100]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-0.6209,  0.0394, -1.7994,  ..., -0.1698, -1.0707,  2.0497],\n",
            "        [ 0.0170,  0.6952, -1.3727,  ..., -1.6491,  1.5005,  0.5282],\n",
            "        [-0.1365, -0.1740,  0.0733,  ..., -0.1379, -0.2114, -0.0928],\n",
            "        ...,\n",
            "        [-0.2540, -0.1234,  0.0551,  ..., -0.4268, -0.3060, -0.0637],\n",
            "        [ 2.1278, -0.6664, -0.2217,  ..., -1.0060,  0.1050,  0.6374],\n",
            "        [ 0.1978,  1.1296, -1.0170,  ..., -0.7425, -0.4788,  0.6417]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-0.0080, -0.1329,  0.1460,  ..., -0.0362, -0.0902, -0.1952],\n",
            "        [ 0.2509,  0.5515, -1.1995,  ..., -1.4708,  1.3147,  0.4417],\n",
            "        [-0.2543, -0.1213,  0.0547,  ..., -0.4226, -0.3080, -0.0654],\n",
            "        ...,\n",
            "        [-0.8775, -0.0395, -0.1840,  ..., -1.2804, -0.9044,  0.2154],\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        [ 0.6149, -0.0976, -1.1774,  ..., -1.2685,  2.1841, -0.0281]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-1.1830,  2.0239, -1.5859,  ..., -0.8078, -0.4302,  0.7737],\n",
            "        [-0.3558,  0.2813, -2.0998,  ..., -0.8452, -0.0525,  1.9655],\n",
            "        [ 0.0046, -0.1581,  0.1543,  ..., -0.0728, -0.0578, -0.1794],\n",
            "        ...,\n",
            "        [ 1.2358, -0.3298, -0.7154,  ..., -1.0132,  1.2807,  0.1145],\n",
            "        [ 0.0091, -0.1596,  0.1546,  ..., -0.0742, -0.0594, -0.1773],\n",
            "        [-1.1284,  2.7407, -1.6941,  ..., -0.7672, -0.7976,  0.5948]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-0.0241, -0.1543,  0.1433,  ..., -0.1121, -0.0854, -0.1666],\n",
            "        [ 0.1095,  0.1581, -0.6179,  ..., -0.8267,  0.8924,  0.0806],\n",
            "        [-0.5931,  0.1785, -2.1314,  ..., -0.3569, -1.0916,  2.4665],\n",
            "        ...,\n",
            "        [-0.4199, -0.1010, -0.0085,  ..., -0.6540, -0.4652,  0.0106],\n",
            "        [-0.0753, -0.2195,  0.0680,  ...,  0.0833, -0.1722, -0.0950],\n",
            "        [ 2.6692, -1.0485, -0.6740,  ..., -1.2922,  1.4063,  0.3923]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[-1.1628,  2.8068, -1.7474,  ..., -0.8161, -0.7927,  0.6333],\n",
            "        [ 0.6722, -0.3808,  0.2036,  ..., -0.2776, -0.2994,  0.1339],\n",
            "        [-2.9047, -0.7550, -2.5088,  ...,  5.4062, -3.5161,  1.0507],\n",
            "        ...,\n",
            "        [-0.0880, -0.2030,  0.0746,  ...,  0.0113, -0.1766, -0.0993],\n",
            "        [ 0.2409, -0.1474,  0.1299,  ...,  0.0226, -0.3340, -0.1014],\n",
            "        [-0.0586, -0.2067,  0.0860,  ...,  0.0508, -0.1483, -0.1126]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-119-f53b1a13eb24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-caab441d932f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-caab441d932f>\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-caab441d932f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    443\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 444\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model"
      ],
      "metadata": {
        "id": "2RDSYL3DGJOa",
        "outputId": "0d6cf981-af77-4252-cf9d-6d4d1861a789",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuantizedModel(\n",
              "  (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "  (dequant): DeQuantize()\n",
              "  (model_fp32): ResNet(\n",
              "    (conv1): QuantizedConv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=1.0, zero_point=0, padding=(3, 3), bias=False)\n",
              "    (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): QuantizedConv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn3): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "          (1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (skip_add): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): QuantizedConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn3): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (skip_add): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): QuantizedConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn3): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (skip_add): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): QuantizedConv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn3): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=1.0, zero_point=0, bias=False)\n",
              "          (1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (skip_add): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): QuantizedConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn3): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (skip_add): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): QuantizedConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn3): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (skip_add): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): QuantizedConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn3): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (skip_add): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): QuantizedConv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn3): QuantizedBatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): QuantizedConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), scale=1.0, zero_point=0, bias=False)\n",
              "          (1): QuantizedBatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (skip_add): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): QuantizedConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn3): QuantizedBatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (skip_add): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): QuantizedConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn3): QuantizedBatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (skip_add): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): QuantizedConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn3): QuantizedBatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (skip_add): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): QuantizedConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn3): QuantizedBatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (skip_add): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): QuantizedConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn3): QuantizedBatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (skip_add): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): QuantizedConv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): QuantizedConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn3): QuantizedBatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): QuantizedConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), scale=1.0, zero_point=0, bias=False)\n",
              "          (1): QuantizedBatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (skip_add): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): QuantizedConv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): QuantizedConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn3): QuantizedBatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (skip_add): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): QuantizedConv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
              "        (bn2): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): QuantizedConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
              "        (bn3): QuantizedBatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (skip_add): QFunctional(\n",
              "          scale=1.0, zero_point=0\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): SingleHeadNestedLinear(\n",
              "      in_features=2048, out_features=10, bias=True\n",
              "      (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "      (dequant): DeQuantize()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UoSc5nBbGcpU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}